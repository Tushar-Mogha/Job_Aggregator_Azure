Updated Project Workflow
1. User Interface
Frontend: Jupyter Notebook with ipywidgets for user input (job/internship, location, category, filters).

App Transformation: Turn notebook into a web app using Voil√†, which displays only widgets and outputs (not code).

2. User Input and Trigger
User interacts with input widgets on the Voil√†-powered app.

Upon submission, user choices (type, location, category) are captured.

3. Data Collection
Web scraping module (e.g., scraper.py in src/) pulls jobs/internships live from Internshala (and others as added) based on user input.

Raw scraped data is stored in data/raw/ for reproducibility if needed.

4. Data Processing
Data cleaning and formatting (via cleaner.py).

Results are structured and optionally filtered/ranked.

Processed datasets may be saved in data/processed/.

5. Cloud Integration (Azure)
Results and/or logs can be uploaded to Azure Storage (azure_upload.py or within storage.py).

Azure authentication and DevOps can be set up if the app needs scaling or controlled user access.

6. Output Visualization
Cleaned results are displayed in a dynamic table or dashboard (output widget, pandas DataFrame display) in the Voil√† app.

Option for users to download/export results (e.g., CSV), stored locally or on Azure Blob.

7. Hosting and Access
Deploy Voil√† app on Azure App Service (Free Tier) so others can access and use your job aggregator via the web.

Requirements and configuration files (requirements.txt, config.py) ensure smooth deployment.

Optionally, deploy a demo on Binder for easy sharing.

8. Expansion Points
Modular codebase allows adding more job boards/platforms.

Automated Azure-based scheduling for background scraping (future scope).

Logs, error handling, and analytics stored/reported via Azure or local files.

Workflow Steps (Summary List)
User Accesses Web App (hosted via Azure, powered by Voil√†)

Provides Search Criteria (interactive ipywidgets GUI)

Scraper Collects Data (Internshala + more platforms)

Cleaner Processes Results (standardizes output)

Displays Output (dynamic, downloadable, shareable)

(Optional) Uploads Data to Azure Storage

Others Can Use the App Freely Online


------------------------------------------------------------------------------------------------------------

üîÑ Workflow Structure ‚Äì Job Aggregator Web App
1. Data Collection Layer (Scraping + API Calls)

Files Involved: src/scraper.py, src/config.py

Scrapes jobs from Internshala, Indeed, LinkedIn (if allowed), Naukri, etc.

Scraping done with requests + BeautifulSoup (lightweight, free).

Schedule scraping with cron job / Azure Function (free tier) ‚Üí runs daily/weekly.

Raw job postings saved in data/raw/ as JSON/CSV.

‚úÖ Value for SDE role: You‚Äôre implementing automated data pipelines.
‚úÖ Value for Analytics: You‚Äôre collecting real-world data at scale.

2. Data Cleaning & Processing Layer

Files Involved: src/cleaner.py, src/config.py

Standardize job fields (title, company, location, salary, skills, posted date).

Remove duplicates, expired jobs, and normalize skill keywords.

Store processed jobs in data/processed/.

‚úÖ SDE: Handling messy real-world data ‚Üí designing cleaning pipelines.
‚úÖ Analytics: Clean dataset ready for dashboards & insights.

3. Data Storage Layer

Files Involved: src/storage.py, src/azure_upload.py

Store jobs in:

Local SQLite (easy + free)

OR Azure Table Storage / Azure Cosmos DB (free tier).

Maintain ‚Äúactive jobs‚Äù list ‚Üí compare with past runs ‚Üí remove expired jobs.

‚úÖ SDE: Database management & consistency checks.
‚úÖ Analytics: Central source of truth for analysis.

4. Analytics & Exploration Layer

Files Involved: notebooks/exploration.ipynb

Jupyter notebooks for:

Salary trends by role/region.

Demand analysis for skills (Python, SQL, Power BI, etc.).

Company-wise job postings.

Export charts to data/processed/plots/.

‚úÖ SDE: Shows ability to work with Jupyter + analytics pipelines.
‚úÖ Analytics: Demonstrates insights generation for business use.

5. API Layer (Backend Service)

Files Involved: src/api.py (new file)

Flask/FastAPI backend that:

Exposes jobs via /jobs endpoint.

Allows filtering by skill, role, location, date.

Runs locally or deployable on Azure App Service (free tier).

‚úÖ SDE: Real backend dev with REST APIs.
‚úÖ Analytics: Makes your processed data queryable.

6. Frontend Layer (Simple, User-Friendly UI)

Files Involved: frontend/ (new folder with app.py or streamlit_app.py)

Use Streamlit (Python-only, no HTML/CSS needed).

Features:

Search/filter jobs.

Show job postings in a card layout.

Display charts (from analytics layer).

‚úÖ SDE: Shows ability to build full-stack apps with minimum web tech.
‚úÖ Analytics: Interactive dashboards for decision-making.

7. Deployment Layer

Azure Free Services:

Scraper automation ‚Üí Azure Function (timer trigger).

Storage ‚Üí Azure Table/Blob Storage.

API/Frontend ‚Üí Azure App Service (free tier).

Local Option: Can run fully offline with SQLite + Streamlit.

üóìÔ∏è Project Timeline (4 Weeks)
Week 1: Core Data Pipeline

Setup repo & file structure.

Build scraper.py (Internshala first).

Save raw job data ‚Üí data/raw/.

Week 2: Cleaning + Storage

Implement cleaner.py.

Create database schema (SQLite/Azure).

Write storage.py to save jobs.

Week 3: Analytics + API

Build Jupyter notebook insights.

Create /jobs API using FastAPI.

Export plots from notebooks.

Week 4: Frontend + Deployment

Build Streamlit UI with filters & charts.

Deploy scraper automation (Azure Function).

Deploy frontend + backend on Azure App Service.
